[[{"l":"Machine Learning","p":["A repository where I hold a bunch of notes on learning ML."]}],[{"l":"Introduction"}],[{"l":"Introduction to Machine Learning"},{"i":"what-is-machine-learning","l":"What is Machine Learning?","p":["Machine learning (ML) is a subset of artificial intelligence (AI) focused on building systems that learn from and make decisions based on data. Unlike traditional programming, where rules are explicitly coded by a developer, ML enables systems to learn and improve from experience without being explicitly programmed."]},{"l":"Machine Learning vs. Artificial Intelligence vs. Deep Learning","p":["Artificial Intelligence (AI): A broad discipline in computer science aiming to create systems capable of intelligent behavior. It encompasses everything from rule-based systems to sophisticated learning algorithms.","Machine Learning (ML): A subset of AI, machine learning algorithms enable computers to learn from and make predictions or decisions based on data. ML is about data and algorithms, not explicit programming of rules.","Deep Learning (DL): A subset of ML, deep learning uses neural networks with many layers (hence \"deep\") to learn from vast amounts of data. It's particularly powerful for tasks like image and speech recognition, but is very data hungry. Given enough compute and data, however, these models are broadly state of the art (SOTA) in most tasks today."]},{"l":"Importance of Machine Learning","p":["Machine learning is reshaping many aspects of our lives. Its applications range from everyday conveniences like recommendation systems (e.g., Netflix, Amazon) to critical uses in healthcare (e.g., predicting diseases, personalized medicine), finance (e.g., fraud detection), and beyond. Its growing importance is primarily due to:","The explosion of data availability","Advances in computational power","Breakthroughs in learning algorithms"]},{"l":"Applications of Machine Learning","p":["The applications of ML are diverse and growing daily:","Healthcare: Diagnostic systems, treatment planning, drug discovery, personalized medicine.","Finance: Fraud detection, algorithmic trading, credit scoring.","Retail: Personalized customer experiences, inventory management, demand forecasting.","Transportation: Autonomous vehicles, route planning, logistics.","Entertainment: Recommendation systems for movies, music, and content.","In the next section, we'll delve deeper into the fundamental concepts of machine learning and explore how these systems learn from data."]}],[{"l":"Overview of the Machine Learning Development Process","p":["The machine learning process is a series of steps taken to build and deploy a model that can make predictions or decisions based on data. Understanding this process is crucial for anyone venturing into the field of machine learning. Here's a high-level overview:"]},{"l":"Defining the Problem","p":["Problem Identification: Understand and define the problem you're trying to solve with machine learning.","Objective Setting: Set clear, measurable objectives for the ML project."]},{"l":"Data Collection","p":["Data Sourcing: Gather data from various sources like databases, files, online repositories, or through sensors and APIs.","Data Quantity and Quality: Ensure the data is sufficient and of high quality for training a reliable model."]},{"l":"Data Preprocessing","p":["Data Cleaning: Handle missing values, incorrect data, and remove duplicates.","Data Transformation: Normalize or scale data, convert categories to numerical values.","Feature Engineering: Create new features from the existing data to improve model performance."]},{"i":"exploratory-data-analysis-eda","l":"Exploratory Data Analysis (EDA)","p":["Statistical Analysis: Perform statistical analysis to understand data distributions and relationships between variables.","Visualization: Use graphs and charts to visualize data for better understanding and insights."]},{"l":"Model Selection","p":["Choosing a Model: Select an appropriate machine learning model based on the problem type (classification, regression, etc.).","Baseline Model: Start with a simple model to establish a baseline performance."]},{"l":"Model Training","p":["Training the Model: Use training data to teach the model to make predictions or decisions.","Parameter Tuning: Adjust model parameters to improve performance."]},{"l":"Model Evaluation","p":["Testing: Evaluate the model's performance on a separate testing dataset.","Validation Techniques: Use techniques like cross-validation to validate model accuracy."]},{"l":"Model Improvement","p":["Refinement: Refine the model based on test results, possibly iterating through model selection and training again.","Feature Selection: Revisit feature engineering to select the most impactful features."]},{"l":"Model Deployment","p":["Deployment: Deploy the model into a production environment.","Integration: Integrate the model with existing systems for real-time predictions or decision-making."]},{"l":"Monitoring and Maintenance","p":["Performance Monitoring: Regularly monitor the model's performance to ensure it remains accurate over time.","Updating the Model: Update or retrain the model as new data becomes available or when performance degrades."]},{"l":"Conclusion","p":["The machine learning process is iterative and requires constant evaluation and refinement. Understanding each step is vital for successfully developing and deploying ML models that can effectively solve real-world problems.","Next: Fundamental Concepts in Machine Learning"]}],[{"l":"Fundamentals of Machine Learning"}],[{"l":"Common Terminology"},{"l":"Training Dataset","p":["The training dataset is the part of the overall dataset that is used to train the model. It contains the input data, its labels, and the target values."]},{"l":"Testing Dataset","p":["The testing dataset is the part of the overall dataset that is used to test the performance of the trained model. This involves using data not seen by the model during training to make predictions and then comparing those predictions to the correct labels."]},{"l":"Validation Dataset","p":["The validation dataset is the part of the overall dataset that is used to fine-tune the model's hyperparameters, select between multiple models or model configurations, as well as to prevent overfitting. You can also use this portion of the dataset to evaluate the model's performance during training."]},{"l":"Ground Truth","p":["Ground truth refers to the actual labels or target values of the dataset, used to compare the model's predictions to the correct answers and evaluate prediction accuracy."]},{"i":"labeltarget","l":"Label/Target","p":["The label or target is the value that the model is predicting."]},{"l":"Pre-processing","p":["Pre-processing involves preparing the data for model training. This step is crucial for shaping the data into a suitable format, especially when using non-ML-specific datasets."]},{"l":"Feature","p":["A feature is an input to the machine learning algorithm. This is typically a column in a 2D dataset."]},{"l":"Data point","p":["A data point is a collection of features that represents a full input to a model. This is typically a row in a 2D dataset."]},{"l":"Numerical","p":["Numerical data points can be counted (e.g., quantitative data like horsepower in a car, or weight in a person)."]},{"l":"Nominal","p":["Nominal data points cannot be counted (e.g., qualitative data like color).","In the example dataset above, total_bill, tip and size are numerical features, while sex, smoker, day, and time are nominal features. Row 0 is a data point. Your target, however, will change depending on what you want to predict. If you wanted to create a model which predicted how much a person would tip based on the data above, tip would be your target."]},{"l":"Decision Surface","p":["The decision surface in a classification model is the boundary that distinguishes between different classes. It's the separator between the orange and blue sections in this graph. Some classification models learn to distinguish between classes by drawing a hyperplane (that line, but in >2 dimensional space) to separate out different classes. These models then use these hyperplanes to make predictions about which data points belong to which classes."]},{"l":"Model Validation","p":["Model validation assesses a trained model's performance on unseen data, using metrics like accuracy, f1 score, recall, etc."]},{"i":"true-positives-tp","l":"True Positives (TP)","p":["The instances correctly predicted as positive."]},{"i":"true-negatives-tn","l":"True Negatives (TN)","p":["The instances correctly predicted as negative."]},{"i":"false-positives-fp","l":"False Positives (FP)","p":["The instances incorrectly predicted as positive."]},{"i":"false-negatives-fn","l":"False Negatives (FN)","p":["The instances incorrectly predicted as negative."]},{"i":"error-err","l":"Error (ERR)","p":["Shows you how often the model gets a prediction wrong."]},{"i":"accuracy-acc","l":"Accuracy (ACC)","p":["Accuracy represents the proportion of correctly predicted items to the total number of predictions made."]},{"i":"precision-pre","l":"Precision (PRE)","p":["The ratio of true positive predictions to the total number of positive predictions (including both true positives and false positives). It answers the question, \"Of all instances classified as positive, how many are actually positive?\""]},{"i":"recall-rec","l":"Recall (REC)"},{"i":"sensitivitytrue-positive-rate","l":"(Sensitivity/True Positive Rate)","p":["The ratio of true positive predictions to the total number of actual positive instances (including both true positives and false negatives). It answers the question, \"Of all actual positive instances, how many are correctly classified by the model?\""]},{"i":"true-negative-rate-tnr","l":"True Negative Rate (TNR)"},{"i":"specificity","l":"(Specificity)","p":["This is the proportion of true negatives out of the total number of negatives. It aims to tell you the same thing recall tells you about true positives, but about true negatives. This would be useful in a situation where false positives have a significant cost, such as a system which tries to identify nuclear attacks. It is calculated as:"]},{"l":"F1 Score","p":["The F1 score is a statistical measure which balances the trade-off between precision and recall. It is particularly useful in scenarios where an equal importance is given to both false positives and false negatives, or when dealing with imbalanced datasets. The F1 score is the harmonic mean of precision and recall, providing a single metric that accounts for both the false positives and false negatives in the model's predictions."]},{"l":"Receiver Operating Characteristic","p":["This is the ratio of the TPR to the FPR. It is typically represented as a curve with TPR plotted on the y-axis and FPR plotted on the x-axis. A model that classifies things at random will show up as a diagonal line across this plot. A good classifier will hug the top left portion of the graph, indicating that it has a high true positive rate, and a low false positive rate. Likewise, a poor curve will be the inverse of this with a high false positive rate, and a low true positive rate. It is calculated as:"]},{"l":"AUC"},{"i":"area-under-the-receiver-operating-characteristic-curve","l":"(Area under the [Receiver Operating Characteristic] curve)","p":["This is a quantitative measurement of the overall performance of the classifier using the ROC curve. It boils the ROC curve down to a single value. A higher value here will indicate a better classifier. This would be calculated, in theory, as the definite integral of the ROC curve, however numerically it is approximated using a Reimann sum typically implemented as the trapezoidal rule."]},{"l":"Cross-validation","p":["Cross-validation is a technique for estimating a model's generalizability to new data. It involves testing the model against subsets of data withheld from the training set in order to collect statistics which can then ve averaged to produce a more realistic assessment of the model's overall performance."]},{"l":"Hyperparameters","p":["Hyperparameters are the adjustable settings of a model, like learning rate or gamma, which are tuned between training runs to enhance performance."]},{"l":"Overfitting","p":["Overfitting occurs when a model is trained to perfectly (or too closely) fit the training data, to the detriment of its performance on new, unseen data."]}]]