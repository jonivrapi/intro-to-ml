[[{"l":"Machine Learning","p":["A repository where I hold a bunch of notes on learning ML."]}],[{"l":"Introduction"}],[{"l":"Introduction to Machine Learning"},{"i":"what-is-machine-learning","l":"What is Machine Learning?","p":["Machine learning (ML) is a subset of artificial intelligence (AI) focused on building systems that learn from and make decisions based on data. Unlike traditional programming, where rules are explicitly coded by a developer, ML enables systems to learn and improve from experience without being explicitly programmed."]},{"l":"Machine Learning vs. Artificial Intelligence vs. Deep Learning","p":["Artificial Intelligence (AI): A broad discipline in computer science aiming to create systems capable of intelligent behavior. It encompasses everything from rule-based systems to sophisticated learning algorithms.","Machine Learning (ML): A subset of AI, machine learning algorithms enable computers to learn from and make predictions or decisions based on data. ML is about data and algorithms, not explicit programming of rules.","Deep Learning (DL): A subset of ML, deep learning uses neural networks with many layers (hence \"deep\") to learn from vast amounts of data. It's particularly powerful for tasks like image and speech recognition, but is very data hungry. Given enough compute and data, however, these models are broadly state of the art (SOTA) in most tasks today."]},{"l":"Importance of Machine Learning","p":["Machine learning is reshaping many aspects of our lives. Its applications range from everyday conveniences like recommendation systems (e.g., Netflix, Amazon) to critical uses in healthcare (e.g., predicting diseases, personalized medicine), finance (e.g., fraud detection), and beyond. Its growing importance is primarily due to:","The explosion of data availability","Advances in computational power","Breakthroughs in learning algorithms"]},{"l":"Applications of Machine Learning","p":["The applications of ML are diverse and growing daily:","Healthcare: Diagnostic systems, treatment planning, drug discovery, personalized medicine.","Finance: Fraud detection, algorithmic trading, credit scoring.","Retail: Personalized customer experiences, inventory management, demand forecasting.","Transportation: Autonomous vehicles, route planning, logistics.","Entertainment: Recommendation systems for movies, music, and content.","In the next section, we'll delve deeper into the fundamental concepts of machine learning and explore how these systems learn from data."]}],[{"l":"Overview of the Machine Learning Development Process","p":["The machine learning process is a series of steps taken to build and deploy a model that can make predictions or decisions based on data. Understanding this process is crucial for anyone venturing into the field of machine learning. Here's a high-level overview:"]},{"l":"Defining the Problem","p":["Problem Identification: Understand and define the problem you're trying to solve with machine learning.","Objective Setting: Set clear, measurable objectives for the ML project."]},{"l":"Data Collection","p":["Data Sourcing: Gather data from various sources like databases, files, online repositories, or through sensors and APIs.","Data Quantity and Quality: Ensure the data is sufficient and of high quality for training a reliable model."]},{"l":"Data Preprocessing","p":["Data Cleaning: Handle missing values, incorrect data, and remove duplicates.","Data Transformation: Normalize or scale data, convert categories to numerical values.","Feature Engineering: Create new features from the existing data to improve model performance."]},{"i":"exploratory-data-analysis-eda","l":"Exploratory Data Analysis (EDA)","p":["Statistical Analysis: Perform statistical analysis to understand data distributions and relationships between variables.","Visualization: Use graphs and charts to visualize data for better understanding and insights."]},{"l":"Model Selection","p":["Choosing a Model: Select an appropriate machine learning model based on the problem type (classification, regression, etc.).","Baseline Model: Start with a simple model to establish a baseline performance."]},{"l":"Model Training","p":["Training the Model: Use training data to teach the model to make predictions or decisions.","Parameter Tuning: Adjust model parameters to improve performance."]},{"l":"Model Evaluation","p":["Testing: Evaluate the model's performance on a separate testing dataset.","Validation Techniques: Use techniques like cross-validation to validate model accuracy."]},{"l":"Model Improvement","p":["Refinement: Refine the model based on test results, possibly iterating through model selection and training again.","Feature Selection: Revisit feature engineering to select the most impactful features."]},{"l":"Model Deployment","p":["Deployment: Deploy the model into a production environment.","Integration: Integrate the model with existing systems for real-time predictions or decision-making."]},{"l":"Monitoring and Maintenance","p":["Performance Monitoring: Regularly monitor the model's performance to ensure it remains accurate over time.","Updating the Model: Update or retrain the model as new data becomes available or when performance degrades."]},{"l":"Conclusion","p":["The machine learning process is iterative and requires constant evaluation and refinement. Understanding each step is vital for successfully developing and deploying ML models that can effectively solve real-world problems.","Next: Fundamentals of Machine Learning"]}],[{"l":"Fundamentals of ML"}],[{"l":"Common Terminology"},{"l":"Training Dataset","p":["The training dataset is the part of the overall dataset that is used to train the model. It contains the input data, its labels, and the target values."]},{"l":"Testing Dataset","p":["The testing dataset is the part of the overall dataset that is used to test the performance of the trained model. This involves using data not seen by the model during training to make predictions and then comparing those predictions to the correct labels."]},{"l":"Validation Dataset","p":["The validation dataset is the part of the overall dataset that is used to fine-tune the model's hyperparameters, select between multiple models or model configurations, as well as to prevent overfitting. You can also use this portion of the dataset to evaluate the model's performance during training."]},{"l":"Ground Truth","p":["Ground truth refers to the actual labels or target values of the dataset, used to compare the model's predictions to the correct answers and evaluate prediction accuracy."]},{"i":"labeltarget","l":"Label/Target","p":["The label or target is the value that the model is predicting."]},{"l":"Pre-processing","p":["Pre-processing involves preparing the data for model training. This step is crucial for shaping the data into a suitable format, especially when using non-ML-specific datasets."]},{"l":"Feature","p":["A feature is an input to the machine learning algorithm. This is typically a column in a 2D dataset."]},{"l":"Data point","p":["A data point is a collection of features that represents a full input to a model. This is typically a row in a 2D dataset."]},{"l":"Numerical","p":["Numerical data points can be counted (e.g., quantitative data like horsepower in a car, or weight in a person)."]},{"l":"Nominal","p":["Nominal data points cannot be counted (e.g., qualitative data like color).","In the example dataset above, total_bill, tip and size are numerical features, while sex, smoker, day, and time are nominal features. Row 0 is a data point. Your target, however, will change depending on what you want to predict. If you wanted to create a model which predicted how much a person would tip based on the data above, tip would be your target."]},{"l":"Decision Surface","p":["The decision surface in a classification model is the boundary that distinguishes between different classes. It's the separator between the orange and blue sections in this graph. Some classification models learn to distinguish between classes by drawing a hyperplane (that line, but in >2 dimensional space) to separate out different classes. These models then use these hyperplanes to make predictions about which data points belong to which classes."]},{"l":"Model Validation","p":["Model validation assesses a trained model's performance on unseen data, using metrics like accuracy, f1 score, recall, etc."]},{"i":"true-positives-tp","l":"True Positives (TP)","p":["The instances correctly predicted as positive."]},{"i":"true-negatives-tn","l":"True Negatives (TN)","p":["The instances correctly predicted as negative."]},{"i":"false-positives-fp","l":"False Positives (FP)","p":["The instances incorrectly predicted as positive."]},{"i":"false-negatives-fn","l":"False Negatives (FN)","p":["The instances incorrectly predicted as negative."]},{"i":"error-err","l":"Error (ERR)","p":["Shows you how often the model gets a prediction wrong."]},{"i":"accuracy-acc","l":"Accuracy (ACC)","p":["Accuracy represents the proportion of correctly predicted items to the total number of predictions made."]},{"i":"precision-pre","l":"Precision (PRE)","p":["The ratio of true positive predictions to the total number of positive predictions (including both true positives and false positives). It answers the question, \"Of all instances classified as positive, how many are actually positive?\""]},{"i":"recall-rec","l":"Recall (REC)"},{"i":"sensitivitytrue-positive-rate","l":"(Sensitivity/True Positive Rate)","p":["The ratio of true positive predictions to the total number of actual positive instances (including both true positives and false negatives). It answers the question, \"Of all actual positive instances, how many are correctly classified by the model?\""]},{"i":"true-negative-rate-tnr","l":"True Negative Rate (TNR)"},{"i":"specificity","l":"(Specificity)","p":["This is the proportion of true negatives out of the total number of negatives. It aims to tell you the same thing recall tells you about true positives, but about true negatives. This would be useful in a situation where false positives have a significant cost, such as a system which tries to identify nuclear attacks. It is calculated as:"]},{"l":"F1 Score","p":["The F1 score is a statistical measure which balances the trade-off between precision and recall. It is particularly useful in scenarios where an equal importance is given to both false positives and false negatives, or when dealing with imbalanced datasets. The F1 score is the harmonic mean of precision and recall, providing a single metric that accounts for both the false positives and false negatives in the model's predictions."]},{"l":"Receiver Operating Characteristic","p":["This is the ratio of the TPR to the FPR. It is typically represented as a curve with TPR plotted on the y-axis and FPR plotted on the x-axis. A model that classifies things at random will show up as a diagonal line across this plot. A good classifier will hug the top left portion of the graph, indicating that it has a high true positive rate, and a low false positive rate. Likewise, a poor curve will be the inverse of this with a high false positive rate, and a low true positive rate. It is calculated as:"]},{"l":"AUC"},{"i":"area-under-the-receiver-operating-characteristic-curve","l":"(Area under the [Receiver Operating Characteristic] curve)","p":["This is a quantitative measurement of the overall performance of the classifier using the ROC curve. It boils the ROC curve down to a single value. A higher value here will indicate a better classifier. This would be calculated, in theory, as the definite integral of the ROC curve, however numerically it is approximated using a Reimann sum typically implemented as the trapezoidal rule."]},{"l":"Cross-validation","p":["Cross-validation is a technique for estimating a model's generalizability to new data. It involves testing the model against subsets of data withheld from the training set in order to collect statistics which can then ve averaged to produce a more realistic assessment of the model's overall performance."]},{"l":"Hyperparameters","p":["Hyperparameters are the adjustable settings of a model, like learning rate or gamma, which are tuned between training runs to enhance performance."]},{"l":"Overfitting","p":["Overfitting occurs when a model is trained to perfectly (or too closely) fit the training data, to the detriment of its performance on new, unseen data."]}],[{"i":"supervised-unsupervised-and-reinforcement-learning","l":"Supervised, Unsupervised, and Reinforcement Learning","p":["Machine Learning (ML) is a vast field with various techniques and approaches. One of the fundamental ways to categorize these approaches is by how an algorithm learns to become more accurate in its predictions. There are three primary types of ML: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. Each type has its unique approach and application areas."]},{"l":"Supervised Learning","p":["Supervised learning is the most common type of machine learning. In this approach, the algorithm learns from labeled training data, helping the model make predictions or decisions based on new, unseen data."]},{"i":"how-it-works","l":"How it Works:","p":["Training Data: Involves input-output pairs. The input data is the data point, and the output is the label.","Learning Process: The algorithm learns by comparing its output with the actual answer and then adjusting itself to improve accuracy.","Examples:","Regression: Predicting continuous values (e.g., house prices).","Classification: Categorizing data into predefined classes (e.g., spam detection in emails)."]},{"l":"Unsupervised Learning","p":["Unsupervised learning deals with data that has no labels. The goal here is to explore the data and find some form of structure or pattern."]},{"i":"how-it-works-1","l":"How it Works:","p":["Training Data: Only the input data is available; there are no corresponding output labels.","Learning Process: The algorithm tries to organize the data in some way to describe its structure. This can mean grouping the data into clusters or finding different ways of looking at complex data.","Examples:","Clustering: Grouping customers based on purchasing behavior.","Dimensionality Reduction: Reducing the number of variables in a dataset while retaining its essential features."]},{"l":"Reinforcement Learning","p":["Reinforcement Learning is a bit different; it is about action and reaction. It’s learning what to do and how to map situations to actions to maximize a reward signal."]},{"i":"how-it-works-2","l":"How it Works:","p":["Training Data: The algorithm interacts with a dynamic environment in which it must perform a certain goal (e.g., driving a car).","Learning Process: The model learns to achieve a goal in an uncertain, potentially complex environment. It uses feedback from its own actions and experiences.","Examples:","Game Playing: AlphaGo, the program that plays Go.","Robot Navigation: Robots learning to navigate through various terrains."]},{"l":"Conclusion","p":["Understanding these three types of machine learning is fundamental to diving deeper into the field. Each type has its unique challenges and requires different approaches and techniques. Other sections of these docs will have concrete examples of these, as well as more in-depth information."]}],[{"i":"a-not-exhaustive-collection-of-the-mathematical-concepts-related-to-ml","l":"A (not) exhaustive collection of the mathematical concepts related to ML"},{"i":"mean-average","l":"Mean (Average)","p":["The mean is the sum of all values divided by the number of values.","Where N is the number of observations and x_i is each individual observation."]},{"l":"Median","p":["The median is the middle value in a data set when the values are arranged in ascending or descending order."]},{"l":"Mode","p":["The mode is the most frequently occurring value in a data set."]},{"l":"Standard Deviation","p":["Standard deviation measures the amount of variation or dispersion in a set of values.","Where \\mu is the mean."]},{"l":"Normal Distribution","p":["The normal distribution is also known as the Gaussian distribution.","Where \\mu is the mean and \\sigma^2 is the variance."]},{"l":"Binomial Distribution","p":["The binomial distribution represents the number of successes in a sequence of independent experiments.","Where n is the number of trials, p is the probability of success, and k is the number of successes."]},{"l":"Pearson Correlation","p":["Correlation measures the strength and direction of a linear relationship between two variables.","Where \\bar{x} and \\bar{y} are the means of the two variables. A score of 1 would indicate a perfect correlation and that high values of x correspond with high values of y. A score or -1 would indicate a perfect inverse correlation and that high values of x correspond with low values of y. A score of 0 would indicate no correlation and that a change in x does not correspond with a change in y."]},{"l":"Covariance","p":["Covariance indicates the direction of the linear relationship between variables.","For a population: \\text{Cov}(x, y) = \\frac{1}{N} \\sum\\_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})","For a sample: \\text{Cov}(x, y) = \\frac{1}{N-1} \\sum\\_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})","Where \\bar{x} and \\bar{y} are the means of the variables x and y, respectively."]},{"l":"Z-test","p":["Used when the data follows a normal distribution, and the population variance is known.","Where \\bar{x} is the sample mean, \\mu is the population mean, \\sigma is the population standard deviation, and n is the sample size."]},{"l":"1-Sample T-test","p":["Used when the population variance is unknown.","Where \\bar{x} is the sample mean, \\mu is the population mean, \\sigma is the population standard deviation, and n is the sample size."]},{"l":"Linear Regression","p":["In linear regression, we model the relationship between two variables by fitting a linear equation to observed data. The formula for a simple linear regression is:","Where Y is the dependent variable, X is the independent variable, \\beta_0 is the y-intercept, \\beta_1 is the slope of the line, and \\epsilon is the error term."]},{"l":"Multiple Regression","p":["Multiple regression is an extension of linear regression into a relationship with more than one independent variable:"]},{"i":"bayes-theorem","l":"Bayes' Theorem","p":["Bayes' Theorem is fundamental to Bayesian statistics:","Where P(A|B) is the probability of A given B, and P(B|A) is the probability of B given A."]},{"l":"Entropy","p":["Entropy is a measure of randomness or uncertainty.","Where H(X) is the entropy of a random variable X and P(x_i) is the probability of each outcome."]}],[{"l":"Assignment 1","p":["We will be working with the Iris Dataset-- typically used as the Hello world! of machine learning projects. This dataset can be found in the sklearn datasets. You can find the jupyter notebook for this assignment here. Find the completed assignment here.","Question 1: How many datapoints are in this dataset?","Question 2: How many columns (features) are there?","Question 3: What are the datatypes of each feature?","Question 4: What is the target feature name?","Question 5: How many target categories are there?","Create a correlation matrix for the features in this dataset.","Create a pairwise plot of all the features in this dataset.","Speculate as to which features would be most useful for classification based on the above two plots.","Set up your training and testing datasets.","Train decision tree, SVM, NB and MLP classifiers.","Report classification accuracies."]}]]