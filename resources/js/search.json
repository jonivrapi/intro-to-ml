[[{"l":"Machine Learning","p":["A repository where I hold a bunch of notes on learning ML."]}],[{"l":"Introduction to Machine Learning"},{"i":"what-is-machine-learning","l":"What is Machine Learning?","p":["Machine learning (ML) is a subset of artificial intelligence (AI) focused on building systems that learn from and make decisions based on data. Unlike traditional programming, where rules are explicitly coded by a developer, ML enables systems to learn and improve from experience without being explicitly programmed."]},{"l":"Machine Learning vs. Artificial Intelligence vs. Deep Learning","p":["Artificial Intelligence (AI): A broad discipline in computer science aiming to create systems capable of intelligent behavior. It encompasses everything from rule-based systems to sophisticated learning algorithms.","Machine Learning (ML): A subset of AI, machine learning algorithms enable computers to learn from and make predictions or decisions based on data. ML is about data and algorithms, not explicit programming of rules.","Deep Learning (DL): A subset of ML, deep learning uses neural networks with many layers (hence \"deep\") to learn from vast amounts of data. It's particularly powerful for tasks like image and speech recognition, but is very data hungry. Given enough compute and data, however, these models are broadly state of the art (SOTA) in most tasks today."]},{"l":"Importance of Machine Learning","p":["Machine learning is reshaping many aspects of our lives. Its applications range from everyday conveniences like recommendation systems (e.g., Netflix, Amazon) to critical uses in healthcare (e.g., predicting diseases, personalized medicine), finance (e.g., fraud detection), and beyond. Its growing importance is primarily due to:","The explosion of data availability","Advances in computational power","Breakthroughs in learning algorithms"]},{"l":"Applications of Machine Learning","p":["The applications of ML are diverse and growing daily:","Healthcare: Diagnostic systems, treatment planning, drug discovery, personalized medicine.","Finance: Fraud detection, algorithmic trading, credit scoring.","Retail: Personalized customer experiences, inventory management, demand forecasting.","Transportation: Autonomous vehicles, route planning, logistics.","Entertainment: Recommendation systems for movies, music, and content.","In the next section, we'll delve deeper into the fundamental concepts of machine learning and explore how these systems learn from data."]}],[{"l":"Overview of the Machine Learning Development Process","p":["The machine learning process is a series of steps taken to build and deploy a model that can make predictions or decisions based on data. Understanding this process is crucial for anyone venturing into the field of machine learning. Here's a high-level overview:"]},{"l":"Defining the Problem","p":["Problem Identification: Understand and define the problem you're trying to solve with machine learning.","Objective Setting: Set clear, measurable objectives for the ML project."]},{"l":"Data Collection","p":["Data Sourcing: Gather data from various sources like databases, files, online repositories, or through sensors and APIs.","Data Quantity and Quality: Ensure the data is sufficient and of high quality for training a reliable model."]},{"l":"Data Preprocessing","p":["Data Cleaning: Handle missing values, incorrect data, and remove duplicates.","Data Transformation: Normalize or scale data, convert categories to numerical values.","Feature Engineering: Create new features from the existing data to improve model performance."]},{"i":"exploratory-data-analysis-eda","l":"Exploratory Data Analysis (EDA)","p":["Statistical Analysis: Perform statistical analysis to understand data distributions and relationships between variables.","Visualization: Use graphs and charts to visualize data for better understanding and insights."]},{"l":"Model Selection","p":["Choosing a Model: Select an appropriate machine learning model based on the problem type (classification, regression, etc.).","Baseline Model: Start with a simple model to establish a baseline performance."]},{"l":"Model Training","p":["Training the Model: Use training data to teach the model to make predictions or decisions.","Parameter Tuning: Adjust model parameters to improve performance."]},{"l":"Model Evaluation","p":["Testing: Evaluate the model's performance on a separate testing dataset.","Validation Techniques: Use techniques like cross-validation to validate model accuracy."]},{"l":"Model Improvement","p":["Refinement: Refine the model based on test results, possibly iterating through model selection and training again.","Feature Selection: Revisit feature engineering to select the most impactful features."]},{"l":"Model Deployment","p":["Deployment: Deploy the model into a production environment.","Integration: Integrate the model with existing systems for real-time predictions or decision-making."]},{"l":"Monitoring and Maintenance","p":["Performance Monitoring: Regularly monitor the model's performance to ensure it remains accurate over time.","Updating the Model: Update or retrain the model as new data becomes available or when performance degrades."]},{"l":"Conclusion","p":["The machine learning process is iterative and requires constant evaluation and refinement. Understanding each step is vital for successfully developing and deploying ML models that can effectively solve real-world problems.","Next: Fundamentals of Machine Learning"]}],[{"l":"Common Terminology"},{"l":"Training Dataset","p":["The training dataset is the part of the overall dataset that is used to train the model. It contains the input data, its labels, and the target values."]},{"l":"Testing Dataset","p":["The testing dataset is the part of the overall dataset that is used to test the performance of the trained model. This involves using data not seen by the model during training to make predictions and then comparing those predictions to the correct labels."]},{"l":"Validation Dataset","p":["The validation dataset is the part of the overall dataset that is used to fine-tune the model's hyperparameters, select between multiple models or model configurations, as well as to prevent overfitting. You can also use this portion of the dataset to evaluate the model's performance during training."]},{"l":"Ground Truth","p":["Ground truth refers to the actual labels or target values of the dataset, used to compare the model's predictions to the correct answers and evaluate prediction accuracy."]},{"i":"labeltarget","l":"Label/Target","p":["The label or target is the value that the model is predicting."]},{"l":"Pre-processing","p":["Pre-processing involves preparing the data for model training. This step is crucial for shaping the data into a suitable format, especially when using non-ML-specific datasets."]},{"l":"Feature","p":["A feature is an input to the machine learning algorithm. This is typically a column in a 2D dataset."]},{"l":"Data point","p":["A data point is a collection of features that represents a full input to a model. This is typically a row in a 2D dataset."]},{"l":"Numerical","p":["Numerical data points can be counted (e.g., quantitative data like horsepower in a car, or weight in a person)."]},{"l":"Nominal","p":["Nominal data points cannot be counted (e.g., qualitative data like color).","In the example dataset above, total_bill, tip and size are numerical features, while sex, smoker, day, and time are nominal features. Row 0 is a data point. Your target, however, will change depending on what you want to predict. If you wanted to create a model which predicted how much a person would tip based on the data above, tip would be your target."]},{"l":"Decision Surface","p":["The decision surface in a classification model is the boundary that distinguishes between different classes. It's the separator between the orange and blue sections in this graph. Some classification models learn to distinguish between classes by drawing a hyperplane (that line, but in >2 dimensional space) to separate out different classes. These models then use these hyperplanes to make predictions about which data points belong to which classes."]},{"l":"Model Validation","p":["Model validation assesses a trained model's performance on unseen data, using metrics like accuracy, f1 score, recall, etc."]},{"i":"true-positives-tp","l":"True Positives (TP)","p":["The instances correctly predicted as positive."]},{"i":"true-negatives-tn","l":"True Negatives (TN)","p":["The instances correctly predicted as negative."]},{"i":"false-positives-fp","l":"False Positives (FP)","p":["The instances incorrectly predicted as positive."]},{"i":"false-negatives-fn","l":"False Negatives (FN)","p":["The instances incorrectly predicted as negative."]},{"i":"error-err","l":"Error (ERR)","p":["Shows you how often the model gets a prediction wrong."]},{"i":"accuracy-acc","l":"Accuracy (ACC)","p":["Accuracy represents the proportion of correctly predicted items to the total number of predictions made."]},{"i":"precision-pre","l":"Precision (PRE)","p":["The ratio of true positive predictions to the total number of positive predictions (including both true positives and false positives). It answers the question, \"Of all instances classified as positive, how many are actually positive?\""]},{"i":"recall-rec","l":"Recall (REC)"},{"i":"sensitivitytrue-positive-rate","l":"(Sensitivity/True Positive Rate)","p":["The ratio of true positive predictions to the total number of actual positive instances (including both true positives and false negatives). It answers the question, \"Of all actual positive instances, how many are correctly classified by the model?\""]},{"i":"true-negative-rate-tnr","l":"True Negative Rate (TNR)"},{"i":"specificity","l":"(Specificity)","p":["This is the proportion of true negatives out of the total number of negatives. It aims to tell you the same thing recall tells you about true positives, but about true negatives. This would be useful in a situation where false positives have a significant cost, such as a system which tries to identify nuclear attacks. It is calculated as:"]},{"l":"F1 Score","p":["The F1 score is a statistical measure which balances the trade-off between precision and recall. It is particularly useful in scenarios where an equal importance is given to both false positives and false negatives, or when dealing with imbalanced datasets. The F1 score is the harmonic mean of precision and recall, providing a single metric that accounts for both the false positives and false negatives in the model's predictions."]},{"l":"Receiver Operating Characteristic","p":["This is the ratio of the TPR to the FPR. It is typically represented as a curve with TPR plotted on the y-axis and FPR plotted on the x-axis. A model that classifies things at random will show up as a diagonal line across this plot. A good classifier will hug the top left portion of the graph, indicating that it has a high true positive rate, and a low false positive rate. Likewise, a poor curve will be the inverse of this with a high false positive rate, and a low true positive rate. It is calculated as:"]},{"l":"AUC"},{"i":"area-under-the-receiver-operating-characteristic-curve","l":"(Area under the [Receiver Operating Characteristic] curve)","p":["This is a quantitative measurement of the overall performance of the classifier using the ROC curve. It boils the ROC curve down to a single value. A higher value here will indicate a better classifier. This would be calculated, in theory, as the definite integral of the ROC curve, however numerically it is approximated using a Reimann sum typically implemented as the trapezoidal rule."]},{"l":"Cross-validation","p":["Cross-validation is a technique for estimating a model's generalizability to new data. It involves testing the model against subsets of data withheld from the training set in order to collect statistics which can then ve averaged to produce a more realistic assessment of the model's overall performance."]},{"l":"Hyperparameters","p":["Hyperparameters are the adjustable settings of a model, like learning rate or gamma, which are tuned between training runs to enhance performance."]},{"l":"Overfitting","p":["Overfitting occurs when a model is trained to perfectly (or too closely) fit the training data, to the detriment of its performance on new, unseen data."]},{"l":"Regularization","p":["Regularization is when you constrain a model to make it simpler in order to reduce the risk of overfitting to the dataset."]},{"l":"Correlation","p":["Correlation is a statistical measure that describes the extent to which two variables change together. If the correlation is high and positive, it means that when one variable increases, the other tends to increase as well. If it's high and negative, one variable increases as the other decreases. A correlation near zero suggests no strong relationship between the variables."]},{"l":"Multicollinearity","p":["Multicollinearity is a statistical phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy. In a more rigorous context, multicollinearity refers to the situation where the design matrix X in a regression model has some approximate or exact linear dependencies among its columns."]},{"l":"Variance","p":["Variance is a statistical measure that quantifies the spread or dispersion of a set of data points. It is used to represent how far each number in the dataset is from the mean and thus from every other number in the set. In more technical terms, variance measures the average squared deviations from the mean of the data points."]},{"l":"Data Leakage","p":["Data leakage occurs when information from outside the training dataset is used to create the model. If you fit a scaler to the entire dataset (including the test set), you are essentially allowing the model to have prior knowledge about the distribution of the test set. This can lead to overly optimistic estimates of the model's performance"]}],[{"l":"Types of Machine Learning","p":["Machine Learning (ML) is a vast field with various techniques and approaches. One of the fundamental ways to categorize these approaches is by how an algorithm learns to become more accurate in its predictions. There are four primary types of ML: Supervised Learning, Semisupervised Learning, Unsupervised Learning, and Reinforcement Learning. Each type has its unique approach and application areas."]},{"l":"Supervised Learning","p":["Supervised learning is the most common type of machine learning. In this approach, the algorithm learns from labeled training data, helping the model make predictions or decisions based on new, unseen data."]},{"i":"how-it-works","l":"How it Works:","p":["Training Data: Involves input-output pairs. The input data is the data point, and the output is the label.","Learning Process: The algorithm learns by comparing its output with the actual answer and then adjusting itself to improve accuracy.","Examples:","Regression: Predicting continuous values (e.g., house prices).","Classification: Categorizing data into predefined classes (e.g., spam detection in emails).","Algorithms","k-Nearest Neighbors","Linear Regression","Logistic Regression","Support Vector Machines (SVMs)","Decision Trees and Random Forests","Neural Networks"]},{"l":"Semisupervised Learning","p":["Semisupervised learning is supervised learning but over partially labelled data. Labeling data, in general, is very time-consuming, and you will frequently find yourself with many unlabeled datapoints and few labeled ones. Some algorithms are capable of dealing with this.","Of these algorithms, most are combinations of unsupervised and supervised algorithms. Google Photos is a good example of this. You upload an entire album of photos, and it is able to recognize that Person A is in photos 1, 2, and 3, while Person B is in photos 3, 4, and 5. Once you tell it that Person A is Bob and Person B is Mary, it is able to automatically label all photos of both people.","Algorithms","Deep Belief Networks (DBNs)"]},{"l":"Unsupervised Learning","p":["Unsupervised learning deals with data that has no labels. The goal here is to explore the data and find some form of structure or pattern."]},{"i":"how-it-works-1","l":"How it Works:","p":["Algorithms","Anomaly Detection and Novelty Detection","Apriori","Association Rule Learning","Clustering","Clustering: Grouping customers based on purchasing behavior.","DBSCAN","Dimensionality Reduction and Visualization","Dimensionality Reduction: Reducing the number of variables in a dataset while retaining its essential features.","Eclat","Examples:","Hierarchical Cluster Analysis (HCA)","Isolation Forest","K-Means","Kernel PCA","Learning Process: The algorithm tries to organize the data in some way to describe its structure. This can mean grouping the data into clusters or finding different ways of looking at complex data.","Locally Linear Embedding (LLE)","One-class SVM","Principal Component Analysis (PCA)","t-Distributed Stochastic Neighbor Embedding (t-SNE)","Training Data: Only the input data is available; there are no corresponding output labels."]},{"l":"Reinforcement Learning","p":["Reinforcement Learning is a bit different; it is about action and reaction. It’s learning what to do and how to map situations to actions to maximize a reward signal."]},{"i":"how-it-works-2","l":"How it Works:","p":["Training Data: The algorithm interacts with a dynamic environment in which it must perform a certain goal (e.g., driving a car).","Learning Process: The model learns to achieve a goal in an uncertain, potentially complex environment. It uses feedback from its own actions and experiences.","Examples:","Game Playing: AlphaGo, the program that plays Go.","Robot Navigation: Robots learning to navigate through various terrains."]},{"l":"Conclusion","p":["Understanding these three types of machine learning is fundamental to diving deeper into the field. Each type has its unique challenges and requires different approaches and techniques. Other sections of these docs will have concrete examples of these, as well as more in-depth information."]}],[{"l":"Types of Learning","p":["Yet another way to categorize machine learning systems is by how they generalize to data it has never seen before. Generalization is typically centered on approaches to learning, two of which are Instance based, and Model based learning."]},{"l":"Instance Based Learning","p":["This may be the most trivial form of learning, and works basically by memorization. Consider the example of a spam filter. If a user flags an email as spam, and you (as the email service operator) know that x other users received the same email, you can go ahead and flag them all.","You could make this system more intelligent by creating some sort of measure of similarity between emails, and then use that to flag similar emails. Suppose you implemented a naive similarity measure that is implemented as a count of words that the original spam email, and others, have in common. If they have above a certain threshold of similar word counts, then you would flag them as spam.","This is the essence of instance-based learning. A system memorizes some examples, and then uses a similarity measure to generalize to new data."]},{"l":"Model Based Learning","p":["Another way to learn is to build a model based on a set of examples and then use that model to make predictions on unseen data. A model is essentially a function that fits some data. For example,","is a linear model with parameters m and b. It describes a model where y is linearly related to x, meaning the relationship between x and y is represented by a straight line in 2D space. By varying m and b, you can represent any linear function.","How do you choose m and b so that your model works well with your dataset? To do this, you need to define a cost function that measures how well your model fits the data. For linear models like the one above, people typically define cost functions that measure the distance between the model's predictions and the actual data points in the training set. Common choices include Mean Squared Error (MSE) or Mean Absolute Error (MAE).","You then aim to minimize this cost function to improve your model's fit. The smaller the distance between your model's predictions and the actual data points, the better your model fits the data, and the more accurate its predictions are. The learning algorithm iteratively adjusts the model parameters m and b in the direction that minimizes the cost function. This iterative optimization is often done using algorithms like Gradient Descent, and this process is how the model 'learns.'"]}],[{"i":"a-not-exhaustive-collection-of-the-mathematical-concepts-related-to-ml","l":"A (not) exhaustive collection of the mathematical concepts related to ML"},{"l":"Notation","p":["A note on typical ML mathematical notation:","Lowercase bold face font is used to refer to vectors \\mathbf{x^{(i)}} where \\mathbf{x} is the vector of values at the i'th row.","Uppercase \\mathbf{X} is used to signify a matrix."]},{"i":"mean-average","l":"Mean (Average)","p":["The mean is the sum of all values divided by the number of values.","Where N is the number of observations and x_i is each individual observation."]},{"l":"Median","p":["The median is the middle value in a data set when the values are arranged in ascending or descending order."]},{"l":"Mode","p":["The mode is the most frequently occurring value in a data set."]},{"l":"Standard Deviation","p":["Standard deviation measures the amount of variation or dispersion in a set of values.","Where \\mu is the mean."]},{"l":"Normal Distribution","p":["The normal distribution is also known as the Gaussian distribution.","Where \\mu is the mean and \\sigma^2 is the variance."]},{"l":"Binomial Distribution","p":["The binomial distribution represents the number of successes in a sequence of independent experiments.","Where n is the number of trials, p is the probability of success, and k is the number of successes."]},{"l":"Pearson Correlation","p":["Correlation measures the strength and direction of a linear relationship between two variables.","Where \\bar{x} and \\bar{y} are the means of the two variables. A score of 1 would indicate a perfect correlation and that high values of x correspond with high values of y. A score or -1 would indicate a perfect inverse correlation and that high values of x correspond with low values of y. A score of 0 would indicate no correlation and that a change in x does not correspond with a change in y."]},{"l":"Covariance","p":["Covariance indicates the direction of the linear relationship between variables.","For a population: \\text{Cov}(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})","For a sample: \\text{Cov}(x, y) = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})","Where \\bar{x} and \\bar{y} are the means of the variables x and y, respectively."]},{"l":"Z-test","p":["Used when the data follows a normal distribution, and the population variance is known.","Where \\bar{x} is the sample mean, \\mu is the population mean, \\sigma is the population standard deviation, and n is the sample size."]},{"l":"1-Sample T-test","p":["Used when the population variance is unknown.","Where \\bar{x} is the sample mean, \\mu is the population mean, \\sigma is the population standard deviation, and n is the sample size."]},{"l":"Linear Regression","p":["In linear regression, we model the relationship between two variables by fitting a linear equation to observed data. The formula for a simple linear regression is:","Where Y is the dependent variable, X is the independent variable, \\beta_0 is the y-intercept, \\beta_1 is the slope of the line, and \\epsilon is the error term."]},{"l":"Multiple Regression","p":["Multiple regression is an extension of linear regression into a relationship with more than one independent variable:"]},{"i":"bayes-theorem","l":"Bayes' Theorem","p":["Bayes' Theorem is fundamental to Bayesian statistics:","Where P(A|B) is the probability of A given B, and P(B|A) is the probability of B given A."]},{"l":"Entropy","p":["Entropy is a measure of randomness or uncertainty.","Where H(X) is the entropy of a random variable X and P(x_i) is the probability of each outcome."]},{"l":"Univariate Regression","p":["Univariate regression is a statistical technique used to model and analyze the relationship between a single independent variable x and a dependent variable y. It aims to fit a linear model to the data, typically represented as y = mx + b, where m is the slope and b is the y-intercept. The goal is to find the best-fitting line through the data points that minimizes the differences (residuals) between the observed values and the values predicted by the model."]},{"l":"Multivariate Regression","p":["Multivariate regression is a statistical technique used to model the relationship between two or more independent variables x₁, x₂, ..., xₙ and a dependent variable y. It aims to fit a linear model to the data, typically represented as y = b₀ + b₁x₁ + b₂x₂ + ... + bₙxₙ, where b₀ is the intercept and b₁, b₂, ..., bₙ are the coefficients of the independent variables. The goal is to find the coefficients that best predict the dependent variable, minimizing the difference between the observed values and the model's predictions, often using methods like least squares."]},{"l":"Multicollinearity","p":["\\beta_0, \\beta_1, ..., \\beta_k are the coefficients of the model.","\\epsilon is the error term.","Consequences of multicollinearity include:","Detecting multicollinearity typically involves looking at correlation matrices, variance inflation factors (VIF), or condition indices.","Difficulty in determining the individual effect of correlated predictors on the dependent variable.","for some predictor variable X_j(where j is between 1 and k), with a high degree of accuracy (i.e., the variance of \\nu, the error term, is small).","Here's a more formal definition:","In the context of a multiple regression model, the model can be represented as:","In this situation, the matrix X^TX(where X is the design matrix of predictor variables) becomes close to singular (or actually singular in the case of perfect multicollinearity), meaning that its determinant is close to zero or exactly zero. This causes problems in estimating the coefficients \\beta_i using ordinary least squares (OLS) as the inverse of X^TX(required in OLS estimation) does not exist or is poorly conditioned.","Inflated variances of the estimated coefficients, leading to wider confidence intervals and less reliable statistical tests.","Multicollinearity is a statistical phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy. In a more rigorous context, multicollinearity refers to the situation where the design matrix X in a regression model has some approximate or exact linear dependencies among its columns.","Multicollinearity is present when:","Unstable estimates of the coefficients, where small changes in the model or the data can lead to large changes in the coefficient estimates.","where:","X_1, X_2, ..., X_k are the independent (predictor) variables.","Y is the dependent variable."]},{"l":"Variance","p":["Variance is a statistical measure that quantifies the spread or dispersion of a set of data points. It is used to represent how far each number in the dataset is from the mean and thus from every other number in the set. In more technical terms, variance measures the average squared deviations from the mean of the data points.","Formally, the variance of a random variable X is defined as the expected value of the squared deviation of X from its mean \\mu, denoted as \\text{Var}(X) or \\sigma^2. Mathematically, it is expressed as:","where:","E is the expected value operator.","X is a random variable.","\\mu is the mean of X.","For a finite set of data points x_1, x_2, ..., x_n, the sample variance is calculated as:","s^2 is the sample variance.","x_i are the individual data points.","\\bar{x} is the sample mean.","n is the number of data points.","The factor n-1 in the denominator is used for an unbiased estimate of the variance in the case of a sample (this is known as Bessel's correction). If the entire population is used, the denominator becomes n.","Variance is a foundational concept in statistics, providing a measure of the variability or spread in a set of data. A low variance indicates that the data points tend to be very close to the mean and hence to each other, while a high variance indicates that the data points are spread out over a wider range of values."]},{"l":"Convex Functions","p":["A function f(x) is called convex on an interval if the line segment between any two points on the graph of the function lies above or on the graph. Formally, a function f: I \\rightarrow \\mathbb{R}(where I is an interval) is convex if, for any two points x_1, x_2 \\in I and any \\lambda such that 0 \\leq \\lambda \\leq 1, the following inequality holds:","In simpler terms, this means that the function curves downwards, or has an upward-facing curvature. Examples include the function f(x) = x^2 or f(x) = e^x. A twice-differentiable function is convex on an interval if its second derivative is non-negative throughout that interval."]},{"l":"Concave Functions","p":["A function is concave on an interval if the line segment between any two points on the graph of the function lies below or on the graph. Formally, a function f: I \\rightarrow \\mathbb{R} is concave if, for any two points x_1, x_2 \\in I and any \\lambda such that 0 \\leq \\lambda \\leq 1, the following inequality holds:","This means that the function curves upwards, or has a downward-facing curvature. Examples include f(x) = \\log(x) or f(x) = -x^2. A twice-differentiable function is concave on an interval if its second derivative is non-positive throughout that interval."]}],[{"l":"End-to-End ML Project","p":["In this article, we will work through a simple end-to-end ML project based on Chapter 2 of Hands-On Machine learning with Scikit-Learn and TensorFlow by Aurélien Géron to give you a taste for how this works in practice. It will closely follow the following structure:","Frame the Problem","Choosing a Performance Measure","Analyze the data to gain insights about the problem","Preprocess the data for ML","Choose a model to test","Train the model","Evaluate the model"]},{"l":"California Housing Data","p":["Content","Context","households","housing_median_age","housing.csv","latitude","longitude","median_house_value","median_income","ocean_proximity","population","The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.","The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned, so there are some preprocessing steps required! The columns are as follows, their names are pretty self-explanatory:","The purpose of this project is to ultimately end up with a model that is able to predict the median house price in any district, give all the other metrics.","This dataset comes from Kaggle and has the following explanation:","This is the dataset used in the second chapter of Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being too toyish and too cumbersome.","total_bedrooms","total_rooms","While there are many toy datasets that you can play with to practice your ML skill set, it is always best to try and work with real data as this is more representative of what you will have to deal with in your day job. Data is never clean, always hard to find, and almost never complete. With that in mind, we will be using the California Housing Prices dataset which you can download here:"]},{"l":"Framing the Problem","p":["The first thing you should do is to ask what the business objective is. This will determine many facets of how you approach the problem, such as which performance measures you choose, which algorithms you select, and how much time you will spend optimizing hyperparameters.","After asking how the model is expected to be used, you are told that this model's output is expected to be fed into another ML system as one signal among many, whose purpose is to predict whether it is worth investing in a given area or not. Cool.","The next question you may ask would be how this process is currently done. You determine that the process is currently manual, and a team of SME's gathers the most recent information about a district, and then uses a complex rule-set to estimate a price. This takes a huge amount of time, and the estimates are mediocre.","Now, given what you have read in previous sections, is this a supervised, unsupervised, or RL problem? Is it a Classification task, or a Regression task? Upon looking at the explanation of the data above, you can see that the median house value is present, making this a textbook supervised learning task. In addition, since you are attempting to predict a continuous value, it is a regression task. Finally, since we are using multiple features to predict this continuous value, it is a multiple regression task. Finally, since we are only trying to predict a single value, it will be a univariate regression problem."]},{"l":"Selecting a Performance Measure","p":["A typical performance measure for regression problems is the Root Mean Squared Error (RMSE). This is a way, as with the Mean Absolute Error (MAE), to measure the distance between two vectors -- in our case, the vector of predictions and the vector of target values. It is defined as follows:","Where","m is the number of instances in your dataset that you are measuring RMSE on","\\mathbf{x^{(i)}} is the vector of all feature values excluding the label of the $i$th instance (row) in the dataset","y^{(i)} is x^{(i)}'th's label","h is your system's prediction function, also called your hypothesis. If given \\mathbf{x^{(i)}} it outputs \\hat{y^{(i)}} = h(\\mathbf{x^{(i)}}).","RMSE corresponds to the Euclidean norm, and is also referred to as the L_2 norm and is denoted as \\|x\\|_2. MAE corresponds to the Manhattan norm and is referred to as the L_1 norm and denoted \\|x\\|_1."]},{"l":"Getting the data","p":["In our case, you will get the data from here:","End-to-End ML Project"]},{"l":"Analyze the data","p":["Next, we will perform some analysis of the data so that we can get a better handle on it. This part of the ML development process is extremely important as it is where you will garner the vast majority of insights you develop from working with your dataset. This will help inform your model development later on.","Let's begin by doing a sanity check of our data.","Next, let's try and get an understanding of how much data we have, and how much of it may be missing.","Here you will notice that there are 20,640 total observations in the dataset, and that all columns except for total_bedrooms have 20,640 rows. total_bedrooms seems to be missing 207 values...which we will handle later. Another important thing to notice here is that all values are numerical, except for ocean_proximity which is of type object(in this case a string). It is therefore a categorical feature, and you can inspect the categories and their counts like so:","Finally, another useful thing to do is to plot your numerical values with a histogram. This can help to visually reveal the distributions and scales of your numerical features which can inform preprocessing steps and model selection down the line.","You may notice here that many of your features are on different scales. This is also a problem that we will handle later on. Many models require that your input features be on the same scale lest they potentially may learn that one feature is more important than another, simply because its values are larger."]},{"l":"Further Data Exploration","p":["We did a basic exploration of the data earlier on this page, and will follow up with slightly more here. The purpose of this section is to keep this whole process as simple as possible to give you an idea for how you might approach things, but remember that there are a LOT of things you can do to explore your data to look for patterns.","The following is an example of how to create a correlation matrix so you are able to see how correlated your features are. This only works with numerical features and for brevity's sake, we excluded the categorical features. There are, however, many ways to handle categorical features will be discussed in a future section.","In reading this chart, a value of 1 indicates a perfect correlation, a value of 0 indicates no correlation, and a value of -1 indicates a perfect negative correlation. Reading this chart, we can see that there is a high correlation between total_rooms and total_bedrooms(which makes sense) as well as between total_rooms and households and population. Likewise, there is a high correlation between households and population. The rest of the numerical features seem to have insignificant correlations. Finally, we can also see that median_income is pretty highly correlated to median_house_value(which is our target). Now, what are you to do with this information?","Well, highly correlated variables can help guide you in your feature selection process. You do not typically want to use many very highly correlated features together as they can reduce the performance of regression models due to multicollinearity. By dropping some of these features, you also may be able to improve a model's predictive performance as well as its compute performance. When features are highly correlated with your target variable, however, this is an indication that they may hold good predictive power. Remember that this matrix is only showing linear correlations. It completely ignores non-linear relationships which are powerful as well."]},{"l":"Handling Missing Values","p":["Next, in order to prep our data for the model, we should first handle any missing values. Let's check if there are any first by reviewing the dataframe info output here. You will notice that total_bedrooms seems to have 217 rows with missing values. One way to handle this is to impute missing values. You can impute missing values in a whole host of different ways, the simplest of which are just to fill them with the mean or median of the values in the column. In thinking about the field total_bedrooms, would the median or the mean be the best solution for imputing missing values here? Considering the mean would likely be a floating point number, and that 3.25 bedrooms doesn't make sense, the median is likely the better option here. Lets see what that looks like:","Here, we instantiate a SimpleImputer and give it a strategy. We then fit this imputer and use it to transform the training and testing dataframes to impute the missing values. Finally, as the result of the transform is a numpy array, we convert it back to a dataframe for easier viewing and manipulation. We can now sanity-check that we did, in fact, solve our nulls issue by rerunning the info command.","Note: It is important to remember that you can only fit the scalars, or any kind of transformation, to the training set in order to prevent data leakage."]},{"l":"Training a Model","p":["The first step in training a model is of course selecting the model. To keep things simple, here we will choose a simple LinearRegression model from sklearn.","We fit the model to our training data, then made predictions on our test data, then evaluated those predictions with our chosen performance measure, and achieved a model which is off by about $71,000 per housing price prediction. This is obviously not good, but for our first end-to-end ML project, not bad!"]}],[{"l":"Assignment 1","p":["We will be working with the Iris Dataset-- typically used as the Hello world! of machine learning projects. This dataset can be found in sklearn datasets. You can find the jupyter notebook for this assignment here. Find the completed assignment here.","Import the dataset and do a sanity check:","Question 1: How many datapoints are in this dataset?","Question 2: How many columns (features) are there?","Question 3: What are the datatypes of each feature?","Question 4: What is the target feature name?","Question 5: How many target categories are there?","Create a correlation matrix for the features in this dataset.","Create a pairwise plot of all the features in this dataset.","Speculate as to which features would be most useful for classification based on the above two plots.","Set up your training and testing datasets.","Train decision tree, SVM, NB and MLP classifiers.","Report classification accuracies."]}],[{"i":"assignment-1---complete","l":"Assignment 1 - Complete","p":["Create a correlation matrix for the features in this dataset.","Create a pairwise plot of all the features in this dataset.","Import the dataset and do a sanity check:","png","Question 1: How many datapoints are in this dataset?","Question 2: How many columns (features) are there?","Question 3: What are the datatypes of each feature?","Question 4: What is the target feature name?","Question 5: How many target categories are there?","Report classification accuracies.","Set up your training and testing datasets.","Speculate as to which features would be most useful for classification based on the above two plots.","The pairplot seems to provide the most useful information here. If you look at the charts across the diagonal of the pairplot, you will notice that (starting from the top left and going down) the first two charts have a serious amount of overlap. Since we are trying to draw (in our heads) a hyperplane to divide the target classes, you will notice that petal length vs petal width share the least amount of overlap and are therefore most easily clustered.","The target is the thing you are trying to predict, in the case of this assignment, that is the species.","Train decision tree, SVM, NB and MLP classifiers.","We will be working with the Iris Dataset-- typically used as the Hello world! of machine learning projects. This dataset can be found in sklearn datasets. You can find the jupyter notebook for this assignment here. Find the completed assignment here."]}]]